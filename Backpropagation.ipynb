{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNz3CB-6TPrQ"
   },
   "source": [
    "# Question-4 : Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQC2CC3HTPra"
   },
   "source": [
    "## Explanation for model structure and assumptions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd5JFmsuTPra"
   },
   "source": [
    "### Vectorized data :\n",
    "- The data has been converted to vector form in order to aid computation time \n",
    "\n",
    "\n",
    "### Train/Validation Split -\n",
    "- A ratio of 80:20 is chosen as it is widely considered a good starting point(adhering to pareto principle) and is performs well with the amount of data at hand\n",
    "\n",
    "\n",
    "### Number of hidden nodes -\n",
    "- A fair bit of number of nodes were tested with 40 giving the best results\n",
    "\n",
    "### Activation function : \n",
    "- In this implemetation of backpropagation non-linear function activation functions are chosen as chossing a linear function would be resundant as there will be no learning\n",
    "- For hidden layer sigmoid function is chosen\n",
    "- For connection between hidden and outplut layer softmax function is chosen\n",
    "- Justification- \n",
    "- Sigmoid function - \n",
    "\n",
    "- 1.This function is takes any input value but the output is always between 0 and 1 \n",
    "- 2. Chooosing sigmoid activation function can result in vanishing/ exploding gradient problem which is addressed in this implementation\n",
    "- 3. As this is a simple implemetation sigmoid function was chosen over other activation functions like ReLu\n",
    "\n",
    "- Softmax function - \n",
    "- 1.Softmax function converts inputs to values between 0 and 1 with the sum of all elements adding up to 1\n",
    "- 2. Softmax funcion is considered a standard for classification problems for output layer\n",
    "\n",
    "\n",
    "### Addressing vanishing gradient problem -\n",
    "- As we are using sigmoid function , vanishing gradient is a problem which needs to be tackled. For this purpose we use Xavier (Glorot) weight initialization scheme which adds a term to wights to keep them from vanishing/exploding.\n",
    "\n",
    "\n",
    "### Loss Function :\n",
    "- Cross entropy has been chosen as the choice of error function and cost as it is one of the most effective for classification problems\n",
    "- Cross entropy loss is the measure of  difference between discovered and the predicted distribution in a classification problem\n",
    "- Cross-entropy loss or takes the sum of negative logarithms of likelihood of each scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaHRAMhFTPrR"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sk2jkEYRTPrS"
   },
   "outputs": [],
   "source": [
    "# importing required libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFVvu_yiTPrT"
   },
   "source": [
    "### Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ws3zgiS9TPrU"
   },
   "outputs": [],
   "source": [
    "# importing features and labels from files\n",
    "features = pd.read_csv(\"./train_data.csv\", header=None)\n",
    "labels = pd.read_csv(\"./train_labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ONYwKrKTPrV"
   },
   "source": [
    "### Dividing data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LZ-P1ftWTPrW"
   },
   "outputs": [],
   "source": [
    "# splitting the data into training and validation in the ratio 80:20\n",
    "split = 0.2\n",
    "row_x,col_x = features.shape\n",
    "row_y,col_y = labels.shape\n",
    "train_data = features.iloc[int(row_x*split):,:]\n",
    "train_labels = labels.iloc[int(row_y*split):,:]\n",
    "val_data = features.iloc[0:int(row_x*split),:]\n",
    "val_labels = labels.iloc[0:int(row_y*split),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LpCYfG1AWPdT"
   },
   "outputs": [],
   "source": [
    "# displaying the resulting shapes of datasets (train and validation) after split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X90wD3YATPrX",
    "outputId": "d1442790-1f62-4ec2-80db-4794146d6433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 19804)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array(train_data.T)\n",
    "train_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSXr-DBhTPrY",
    "outputId": "f0369cc3-7d38-4423-9363-8307f3ae6f23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19804)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = np.array(train_labels.T)\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfRASlZbcHhf",
    "outputId": "f68c008d-6149-464f-fada-18f36b134406"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 4950)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = np.array(val_data.T)\n",
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sNQwArLcJTS",
    "outputId": "53e9b79e-1567-4047-d795-1e6a82ecfcd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels = np.array(val_labels.T)\n",
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSKIgK59TPrc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VtSpbVasTPrd"
   },
   "outputs": [],
   "source": [
    "# Initializing parametes - weights and biases aas well as shuffling them\n",
    "# initial biases are taken as 0\n",
    "\n",
    "def define_parameters(inp_nodes, hidden_nodes, out_nodes):\n",
    "    \n",
    "    \n",
    "    initial_Weight1=np.random.randn(hidden_nodes, inp_nodes) * np.sqrt(1/(inp_nodes))\n",
    "    initial_bias1=np.zeros((hidden_nodes, 1))\n",
    "    initial_Weight2=np.random.randn(out_nodes, hidden_nodes) * np.sqrt(1/(hidden_nodes))\n",
    "    initial_bias2=np.zeros((out_nodes, 1))\n",
    "    \n",
    "    params = {\"Weight1\": initial_Weight1,\n",
    "                \"bias1\": initial_bias1,\n",
    "                \"Weight2\": initial_Weight2,\n",
    "                \"bias2\": initial_bias2 }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swytm_AoTPre"
   },
   "source": [
    "The issue of vanishing gradient is addressed by using Xavier weight initialization wherein random wights were multiplied by square root of ( 1/ number of input connections to that layer ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l11ipzlyTPrf"
   },
   "outputs": [],
   "source": [
    "# activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kmo6EddmTPrf"
   },
   "outputs": [],
   "source": [
    "# activation function for output layer\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qQAtjWwvTPrg"
   },
   "outputs": [],
   "source": [
    "# Function for forward propagation ( containing dot product and activation function)\n",
    "\n",
    "def forw_prop(X, params):\n",
    "\n",
    "    Z1= np.dot(params[\"Weight1\"],X) + params[\"bias1\"] \n",
    "    Activ_func1= sigmoid(Z1)\n",
    "    Z2= np.dot(params[\"Weight2\"],Activ_func1) + params[\"bias2\"]\n",
    "    Activ_func2= softmax(Z2)\n",
    "    \n",
    "    deravatives = {\"Z1\": Z1,\n",
    "            \"Activ_func1\": Activ_func1,\n",
    "            \"Z2\": Z2,\n",
    "            \"Activ_func2\": Activ_func2 }\n",
    "    \n",
    "    return deravatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "g_vdGzn9TPrg"
   },
   "outputs": [],
   "source": [
    "# Function for cross entropy calculating the loss/ cost for the run with formula - F(X) = â€“ sum x in X G(x) * log(G(x))\n",
    "\n",
    "def cross_entropy(Activ_func2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = (1/m)*np.sum(-Y * np.log(Activ_func2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5aeiIu7BTPrh"
   },
   "outputs": [],
   "source": [
    "# Function for back propagation (calculating wights and bias derivative and performing dot product)\n",
    "\n",
    "def back_prop(params, deravatives, X, Y):  \n",
    "    \n",
    "    deri2 = (1/X.shape[1]) * np.subtract(deravatives[\"Activ_func2\"],Y) \n",
    "    dWeight2 = np.dot(deri2,deravatives[\"Activ_func1\"].T)  \n",
    "    dbias2 = np.sum(deri2, axis=1, keepdims=True)\n",
    "    deri1 = (np.dot(params[\"Weight2\"].T,deri2))*(deravatives[\"Activ_func1\"] - np.power(deravatives[\"Activ_func1\"], 2)) \n",
    "    dWeight1 = np.dot(deri1,X.T)\n",
    "    dbias1 = np.sum(deri1,axis=1, keepdims=True)\n",
    "    \n",
    "\n",
    "    gradients = {\"dWeight1\": dWeight1,\n",
    "    \"dbias1\": dbias1,\n",
    "    \"dWeight2\": dWeight2,\n",
    "    \"dbias2\": dbias2 }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5tKJ89bgTPrh"
   },
   "outputs": [],
   "source": [
    "# Function to update the values (weights and biases) after back propagation run\n",
    "\n",
    "def updating_values(params, gradients, learning_rate = 0.05):\n",
    "\n",
    "    Weight1= params[\"Weight1\"] - (learning_rate*gradients[\"dWeight1\"])\n",
    "    bias1= params[\"bias1\"] - (learning_rate*gradients[\"dbias1\"])\n",
    "    Weight2= params[\"Weight2\"] - (learning_rate*gradients[\"dWeight2\"])\n",
    "    bias2= params[\"bias2\"] - (learning_rate*gradients[\"dbias2\"])\n",
    "    \n",
    "    params = {\"Weight1\": Weight1,\n",
    "    \"bias1\": bias1,\n",
    "    \"Weight2\": Weight2,\n",
    "    \"bias2\": bias2 }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhMKMFkGTPri"
   },
   "source": [
    "0.05 was chosen as the learning rate after a number of trial runs and plottig them against loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lBzeXQSTPri"
   },
   "source": [
    "### calculating and displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jhSIAW62TPrj"
   },
   "outputs": [],
   "source": [
    "# Converting to one-hot encodings for final predicted output\n",
    "def encode(y):   \n",
    "    li = list()\n",
    "    for i in y:\n",
    "        temp = [0, 0, 0, 0]\n",
    "        temp[i] = 1\n",
    "        li.append(temp)\n",
    "        \n",
    "    return np.array(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCb45KtlTPrj"
   },
   "source": [
    "### Importing file acc_calc in order to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_6w_GEYNTPrj"
   },
   "outputs": [],
   "source": [
    "from acc_calc import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeY-h9xkTPrk"
   },
   "source": [
    "## Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LtiqGrg-TPrk"
   },
   "outputs": [],
   "source": [
    "# Initializing and calling all the functions described above \n",
    "\n",
    "def model(X, Y, n_h, num_iterations = 10000):\n",
    "    # initializing hyperparameters\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    n_h=n_h\n",
    "    \n",
    "    parameters = define_parameters(n_x, n_h, n_y)\n",
    "    accuracy_matrix = []\n",
    "    loss_matrix = []\n",
    "    \n",
    "    # looping over iterations in a cycle of forward propagation - result - backward propagation - updating weights and bias and lastly predictions and accuracy\n",
    "    for i in range(0, num_iterations+1):\n",
    "        deravatives = forw_prop(X, parameters)\n",
    "        loss = cross_entropy(deravatives[\"Activ_func2\"], Y)\n",
    "        grads = back_prop(parameters, deravatives, X, Y)\n",
    "        parameters = updating_values(parameters, grads)\n",
    "        deravatives = forw_prop(val_data, parameters)\n",
    "        predictions = np.argmax(deravatives[\"Activ_func2\"], axis=0)\n",
    "        preds = encode(predictions)\n",
    "        accuracy_val = accuracy(val_labels.T, preds)\n",
    "        accuracy_matrix.append(accuracy_val)\n",
    "        loss_matrix.append(loss)\n",
    "        print(\"Accuracy after iteration %i: %f\" %(i, accuracy_val)) \n",
    "        print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "        \n",
    "        \n",
    "    return parameters, accuracy_matrix, loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rOaXsUVTPrk",
    "outputId": "b2b17ad5-40aa-4e69-9625-2e8805e2be6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration 0: 0.332727\n",
      "Cost after iteration 0: 1.404306\n",
      "Accuracy after iteration 1: 0.349697\n",
      "Cost after iteration 1: 1.394664\n",
      "Accuracy after iteration 2: 0.365051\n",
      "Cost after iteration 2: 1.385411\n",
      "Accuracy after iteration 3: 0.382626\n",
      "Cost after iteration 3: 1.376448\n",
      "Accuracy after iteration 4: 0.400606\n",
      "Cost after iteration 4: 1.367706\n",
      "Accuracy after iteration 5: 0.435556\n",
      "Cost after iteration 5: 1.359136\n",
      "Accuracy after iteration 6: 0.475960\n",
      "Cost after iteration 6: 1.350705\n",
      "Accuracy after iteration 7: 0.521414\n",
      "Cost after iteration 7: 1.342388\n",
      "Accuracy after iteration 8: 0.571313\n",
      "Cost after iteration 8: 1.334168\n",
      "Accuracy after iteration 9: 0.602222\n",
      "Cost after iteration 9: 1.326033\n",
      "Accuracy after iteration 10: 0.637172\n",
      "Cost after iteration 10: 1.317972\n",
      "Accuracy after iteration 11: 0.662222\n",
      "Cost after iteration 11: 1.309978\n",
      "Accuracy after iteration 12: 0.685859\n",
      "Cost after iteration 12: 1.302045\n",
      "Accuracy after iteration 13: 0.702828\n",
      "Cost after iteration 13: 1.294168\n",
      "Accuracy after iteration 14: 0.719192\n",
      "Cost after iteration 14: 1.286343\n",
      "Accuracy after iteration 15: 0.733939\n",
      "Cost after iteration 15: 1.278566\n",
      "Accuracy after iteration 16: 0.745051\n",
      "Cost after iteration 16: 1.270836\n",
      "Accuracy after iteration 17: 0.754343\n",
      "Cost after iteration 17: 1.263148\n",
      "Accuracy after iteration 18: 0.765657\n",
      "Cost after iteration 18: 1.255500\n",
      "Accuracy after iteration 19: 0.773333\n",
      "Cost after iteration 19: 1.247891\n",
      "Accuracy after iteration 20: 0.782222\n",
      "Cost after iteration 20: 1.240318\n",
      "Accuracy after iteration 21: 0.788889\n",
      "Cost after iteration 21: 1.232779\n",
      "Accuracy after iteration 22: 0.795556\n",
      "Cost after iteration 22: 1.225273\n",
      "Accuracy after iteration 23: 0.801616\n",
      "Cost after iteration 23: 1.217799\n",
      "Accuracy after iteration 24: 0.808687\n",
      "Cost after iteration 24: 1.210354\n",
      "Accuracy after iteration 25: 0.814747\n",
      "Cost after iteration 25: 1.202937\n",
      "Accuracy after iteration 26: 0.817576\n",
      "Cost after iteration 26: 1.195549\n",
      "Accuracy after iteration 27: 0.822626\n",
      "Cost after iteration 27: 1.188186\n",
      "Accuracy after iteration 28: 0.827273\n",
      "Cost after iteration 28: 1.180849\n",
      "Accuracy after iteration 29: 0.829697\n",
      "Cost after iteration 29: 1.173537\n",
      "Accuracy after iteration 30: 0.833131\n",
      "Cost after iteration 30: 1.166249\n",
      "Accuracy after iteration 31: 0.835152\n",
      "Cost after iteration 31: 1.158984\n",
      "Accuracy after iteration 32: 0.837980\n",
      "Cost after iteration 32: 1.151742\n",
      "Accuracy after iteration 33: 0.841414\n",
      "Cost after iteration 33: 1.144523\n",
      "Accuracy after iteration 34: 0.844848\n",
      "Cost after iteration 34: 1.137325\n",
      "Accuracy after iteration 35: 0.847879\n",
      "Cost after iteration 35: 1.130150\n",
      "Accuracy after iteration 36: 0.849091\n",
      "Cost after iteration 36: 1.122996\n",
      "Accuracy after iteration 37: 0.851111\n",
      "Cost after iteration 37: 1.115864\n",
      "Accuracy after iteration 38: 0.853737\n",
      "Cost after iteration 38: 1.108753\n",
      "Accuracy after iteration 39: 0.855960\n",
      "Cost after iteration 39: 1.101664\n",
      "Accuracy after iteration 40: 0.857980\n",
      "Cost after iteration 40: 1.094597\n",
      "Accuracy after iteration 41: 0.859798\n",
      "Cost after iteration 41: 1.087552\n",
      "Accuracy after iteration 42: 0.861414\n",
      "Cost after iteration 42: 1.080529\n",
      "Accuracy after iteration 43: 0.862626\n",
      "Cost after iteration 43: 1.073528\n",
      "Accuracy after iteration 44: 0.864040\n",
      "Cost after iteration 44: 1.066549\n",
      "Accuracy after iteration 45: 0.866263\n",
      "Cost after iteration 45: 1.059594\n",
      "Accuracy after iteration 46: 0.867677\n",
      "Cost after iteration 46: 1.052663\n",
      "Accuracy after iteration 47: 0.869495\n",
      "Cost after iteration 47: 1.045755\n",
      "Accuracy after iteration 48: 0.870909\n",
      "Cost after iteration 48: 1.038871\n",
      "Accuracy after iteration 49: 0.872525\n",
      "Cost after iteration 49: 1.032012\n",
      "Accuracy after iteration 50: 0.873737\n",
      "Cost after iteration 50: 1.025179\n",
      "Accuracy after iteration 51: 0.874949\n",
      "Cost after iteration 51: 1.018371\n",
      "Accuracy after iteration 52: 0.876162\n",
      "Cost after iteration 52: 1.011590\n",
      "Accuracy after iteration 53: 0.877576\n",
      "Cost after iteration 53: 1.004836\n",
      "Accuracy after iteration 54: 0.879192\n",
      "Cost after iteration 54: 0.998109\n",
      "Accuracy after iteration 55: 0.880404\n",
      "Cost after iteration 55: 0.991411\n",
      "Accuracy after iteration 56: 0.880606\n",
      "Cost after iteration 56: 0.984741\n",
      "Accuracy after iteration 57: 0.881414\n",
      "Cost after iteration 57: 0.978100\n",
      "Accuracy after iteration 58: 0.881818\n",
      "Cost after iteration 58: 0.971490\n",
      "Accuracy after iteration 59: 0.882222\n",
      "Cost after iteration 59: 0.964910\n",
      "Accuracy after iteration 60: 0.883030\n",
      "Cost after iteration 60: 0.958361\n",
      "Accuracy after iteration 61: 0.883838\n",
      "Cost after iteration 61: 0.951844\n",
      "Accuracy after iteration 62: 0.885455\n",
      "Cost after iteration 62: 0.945359\n",
      "Accuracy after iteration 63: 0.887071\n",
      "Cost after iteration 63: 0.938907\n",
      "Accuracy after iteration 64: 0.888081\n",
      "Cost after iteration 64: 0.932488\n",
      "Accuracy after iteration 65: 0.888889\n",
      "Cost after iteration 65: 0.926103\n",
      "Accuracy after iteration 66: 0.889293\n",
      "Cost after iteration 66: 0.919753\n",
      "Accuracy after iteration 67: 0.890707\n",
      "Cost after iteration 67: 0.913438\n",
      "Accuracy after iteration 68: 0.891717\n",
      "Cost after iteration 68: 0.907159\n",
      "Accuracy after iteration 69: 0.892727\n",
      "Cost after iteration 69: 0.900915\n",
      "Accuracy after iteration 70: 0.893939\n",
      "Cost after iteration 70: 0.894709\n",
      "Accuracy after iteration 71: 0.894141\n",
      "Cost after iteration 71: 0.888539\n",
      "Accuracy after iteration 72: 0.894141\n",
      "Cost after iteration 72: 0.882407\n",
      "Accuracy after iteration 73: 0.894949\n",
      "Cost after iteration 73: 0.876312\n",
      "Accuracy after iteration 74: 0.895758\n",
      "Cost after iteration 74: 0.870256\n",
      "Accuracy after iteration 75: 0.897778\n",
      "Cost after iteration 75: 0.864239\n",
      "Accuracy after iteration 76: 0.897778\n",
      "Cost after iteration 76: 0.858261\n",
      "Accuracy after iteration 77: 0.898182\n",
      "Cost after iteration 77: 0.852322\n",
      "Accuracy after iteration 78: 0.898990\n",
      "Cost after iteration 78: 0.846423\n",
      "Accuracy after iteration 79: 0.900202\n",
      "Cost after iteration 79: 0.840564\n",
      "Accuracy after iteration 80: 0.900808\n",
      "Cost after iteration 80: 0.834746\n",
      "Accuracy after iteration 81: 0.901818\n",
      "Cost after iteration 81: 0.828968\n",
      "Accuracy after iteration 82: 0.902424\n",
      "Cost after iteration 82: 0.823231\n",
      "Accuracy after iteration 83: 0.902828\n",
      "Cost after iteration 83: 0.817536\n",
      "Accuracy after iteration 84: 0.903636\n",
      "Cost after iteration 84: 0.811882\n",
      "Accuracy after iteration 85: 0.904646\n",
      "Cost after iteration 85: 0.806269\n",
      "Accuracy after iteration 86: 0.905051\n",
      "Cost after iteration 86: 0.800698\n",
      "Accuracy after iteration 87: 0.905455\n",
      "Cost after iteration 87: 0.795169\n",
      "Accuracy after iteration 88: 0.905657\n",
      "Cost after iteration 88: 0.789682\n",
      "Accuracy after iteration 89: 0.906465\n",
      "Cost after iteration 89: 0.784238\n",
      "Accuracy after iteration 90: 0.907071\n",
      "Cost after iteration 90: 0.778835\n",
      "Accuracy after iteration 91: 0.907273\n",
      "Cost after iteration 91: 0.773476\n",
      "Accuracy after iteration 92: 0.908687\n",
      "Cost after iteration 92: 0.768158\n",
      "Accuracy after iteration 93: 0.908687\n",
      "Cost after iteration 93: 0.762883\n",
      "Accuracy after iteration 94: 0.909091\n",
      "Cost after iteration 94: 0.757651\n",
      "Accuracy after iteration 95: 0.909899\n",
      "Cost after iteration 95: 0.752462\n",
      "Accuracy after iteration 96: 0.909495\n",
      "Cost after iteration 96: 0.747315\n",
      "Accuracy after iteration 97: 0.910101\n",
      "Cost after iteration 97: 0.742211\n",
      "Accuracy after iteration 98: 0.910303\n",
      "Cost after iteration 98: 0.737149\n",
      "Accuracy after iteration 99: 0.910909\n",
      "Cost after iteration 99: 0.732130\n",
      "Accuracy after iteration 100: 0.911111\n",
      "Cost after iteration 100: 0.727154\n",
      "Accuracy after iteration 101: 0.911111\n",
      "Cost after iteration 101: 0.722220\n",
      "Accuracy after iteration 102: 0.911313\n",
      "Cost after iteration 102: 0.717329\n",
      "Accuracy after iteration 103: 0.911717\n",
      "Cost after iteration 103: 0.712480\n",
      "Accuracy after iteration 104: 0.911717\n",
      "Cost after iteration 104: 0.707674\n",
      "Accuracy after iteration 105: 0.912121\n",
      "Cost after iteration 105: 0.702909\n",
      "Accuracy after iteration 106: 0.912525\n",
      "Cost after iteration 106: 0.698187\n",
      "Accuracy after iteration 107: 0.912727\n",
      "Cost after iteration 107: 0.693507\n",
      "Accuracy after iteration 108: 0.913333\n",
      "Cost after iteration 108: 0.688869\n",
      "Accuracy after iteration 109: 0.914141\n",
      "Cost after iteration 109: 0.684272\n",
      "Accuracy after iteration 110: 0.914343\n",
      "Cost after iteration 110: 0.679717\n",
      "Accuracy after iteration 111: 0.914545\n",
      "Cost after iteration 111: 0.675204\n",
      "Accuracy after iteration 112: 0.914747\n",
      "Cost after iteration 112: 0.670732\n",
      "Accuracy after iteration 113: 0.915354\n",
      "Cost after iteration 113: 0.666301\n",
      "Accuracy after iteration 114: 0.915354\n",
      "Cost after iteration 114: 0.661911\n",
      "Accuracy after iteration 115: 0.915556\n",
      "Cost after iteration 115: 0.657561\n",
      "Accuracy after iteration 116: 0.915960\n",
      "Cost after iteration 116: 0.653252\n",
      "Accuracy after iteration 117: 0.916162\n",
      "Cost after iteration 117: 0.648984\n",
      "Accuracy after iteration 118: 0.916364\n",
      "Cost after iteration 118: 0.644756\n",
      "Accuracy after iteration 119: 0.916364\n",
      "Cost after iteration 119: 0.640567\n",
      "Accuracy after iteration 120: 0.916566\n",
      "Cost after iteration 120: 0.636418\n",
      "Accuracy after iteration 121: 0.916970\n",
      "Cost after iteration 121: 0.632309\n",
      "Accuracy after iteration 122: 0.917172\n",
      "Cost after iteration 122: 0.628239\n",
      "Accuracy after iteration 123: 0.917576\n",
      "Cost after iteration 123: 0.624208\n",
      "Accuracy after iteration 124: 0.917778\n",
      "Cost after iteration 124: 0.620216\n",
      "Accuracy after iteration 125: 0.917778\n",
      "Cost after iteration 125: 0.616263\n",
      "Accuracy after iteration 126: 0.917778\n",
      "Cost after iteration 126: 0.612347\n",
      "Accuracy after iteration 127: 0.918384\n",
      "Cost after iteration 127: 0.608470\n",
      "Accuracy after iteration 128: 0.918586\n",
      "Cost after iteration 128: 0.604631\n",
      "Accuracy after iteration 129: 0.918586\n",
      "Cost after iteration 129: 0.600829\n",
      "Accuracy after iteration 130: 0.918586\n",
      "Cost after iteration 130: 0.597064\n",
      "Accuracy after iteration 131: 0.919192\n",
      "Cost after iteration 131: 0.593336\n",
      "Accuracy after iteration 132: 0.919192\n",
      "Cost after iteration 132: 0.589645\n",
      "Accuracy after iteration 133: 0.919192\n",
      "Cost after iteration 133: 0.585991\n",
      "Accuracy after iteration 134: 0.919192\n",
      "Cost after iteration 134: 0.582372\n",
      "Accuracy after iteration 135: 0.919596\n",
      "Cost after iteration 135: 0.578790\n",
      "Accuracy after iteration 136: 0.920404\n",
      "Cost after iteration 136: 0.575243\n",
      "Accuracy after iteration 137: 0.920606\n",
      "Cost after iteration 137: 0.571731\n",
      "Accuracy after iteration 138: 0.920808\n",
      "Cost after iteration 138: 0.568255\n",
      "Accuracy after iteration 139: 0.921818\n",
      "Cost after iteration 139: 0.564813\n",
      "Accuracy after iteration 140: 0.922020\n",
      "Cost after iteration 140: 0.561406\n",
      "Accuracy after iteration 141: 0.922424\n",
      "Cost after iteration 141: 0.558032\n",
      "Accuracy after iteration 142: 0.923232\n",
      "Cost after iteration 142: 0.554693\n",
      "Accuracy after iteration 143: 0.923434\n",
      "Cost after iteration 143: 0.551388\n",
      "Accuracy after iteration 144: 0.923434\n",
      "Cost after iteration 144: 0.548115\n",
      "Accuracy after iteration 145: 0.923636\n",
      "Cost after iteration 145: 0.544876\n",
      "Accuracy after iteration 146: 0.923636\n",
      "Cost after iteration 146: 0.541669\n",
      "Accuracy after iteration 147: 0.924646\n",
      "Cost after iteration 147: 0.538495\n",
      "Accuracy after iteration 148: 0.925455\n",
      "Cost after iteration 148: 0.535353\n",
      "Accuracy after iteration 149: 0.925859\n",
      "Cost after iteration 149: 0.532243\n",
      "Accuracy after iteration 150: 0.926263\n",
      "Cost after iteration 150: 0.529164\n",
      "Accuracy after iteration 151: 0.926667\n",
      "Cost after iteration 151: 0.526116\n",
      "Accuracy after iteration 152: 0.926869\n",
      "Cost after iteration 152: 0.523100\n",
      "Accuracy after iteration 153: 0.926869\n",
      "Cost after iteration 153: 0.520114\n",
      "Accuracy after iteration 154: 0.927273\n",
      "Cost after iteration 154: 0.517158\n",
      "Accuracy after iteration 155: 0.927475\n",
      "Cost after iteration 155: 0.514233\n",
      "Accuracy after iteration 156: 0.927475\n",
      "Cost after iteration 156: 0.511337\n",
      "Accuracy after iteration 157: 0.927475\n",
      "Cost after iteration 157: 0.508471\n",
      "Accuracy after iteration 158: 0.927475\n",
      "Cost after iteration 158: 0.505634\n",
      "Accuracy after iteration 159: 0.927475\n",
      "Cost after iteration 159: 0.502826\n",
      "Accuracy after iteration 160: 0.927475\n",
      "Cost after iteration 160: 0.500046\n",
      "Accuracy after iteration 161: 0.927475\n",
      "Cost after iteration 161: 0.497295\n",
      "Accuracy after iteration 162: 0.927475\n",
      "Cost after iteration 162: 0.494572\n",
      "Accuracy after iteration 163: 0.927879\n",
      "Cost after iteration 163: 0.491876\n",
      "Accuracy after iteration 164: 0.928485\n",
      "Cost after iteration 164: 0.489208\n",
      "Accuracy after iteration 165: 0.928485\n",
      "Cost after iteration 165: 0.486567\n",
      "Accuracy after iteration 166: 0.928485\n",
      "Cost after iteration 166: 0.483953\n",
      "Accuracy after iteration 167: 0.929091\n",
      "Cost after iteration 167: 0.481365\n",
      "Accuracy after iteration 168: 0.929091\n",
      "Cost after iteration 168: 0.478804\n",
      "Accuracy after iteration 169: 0.929091\n",
      "Cost after iteration 169: 0.476269\n",
      "Accuracy after iteration 170: 0.929091\n",
      "Cost after iteration 170: 0.473760\n",
      "Accuracy after iteration 171: 0.929495\n",
      "Cost after iteration 171: 0.471276\n",
      "Accuracy after iteration 172: 0.929495\n",
      "Cost after iteration 172: 0.468817\n",
      "Accuracy after iteration 173: 0.929697\n",
      "Cost after iteration 173: 0.466383\n",
      "Accuracy after iteration 174: 0.929697\n",
      "Cost after iteration 174: 0.463974\n",
      "Accuracy after iteration 175: 0.930101\n",
      "Cost after iteration 175: 0.461589\n",
      "Accuracy after iteration 176: 0.930303\n",
      "Cost after iteration 176: 0.459228\n",
      "Accuracy after iteration 177: 0.930303\n",
      "Cost after iteration 177: 0.456892\n",
      "Accuracy after iteration 178: 0.930303\n",
      "Cost after iteration 178: 0.454578\n",
      "Accuracy after iteration 179: 0.930303\n",
      "Cost after iteration 179: 0.452289\n",
      "Accuracy after iteration 180: 0.930303\n",
      "Cost after iteration 180: 0.450022\n"
     ]
    }
   ],
   "source": [
    "parameters, accuracy_matrix, loss_matrix = model(train_data, train_labels, 45, num_iterations=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19MRGyXGHxDw"
   },
   "source": [
    " here we can observe that with every iteration accuracy is increasing and cost of function is decreasing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoD3h3UpTPrl"
   },
   "source": [
    "#### Visualizing Accuracy vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Dh9c4mxqTPrl",
    "outputId": "640e3b43-c756-4b71-9d3d-9c0ff0cbc981"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,181), accuracy_matrix)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Loss (cross-entropy) vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,181), loss_matrix)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtbLSg_HxDx"
   },
   "source": [
    "It is observed that the accuracy increases exponenttially initially before gradually the curve becoming less steep . It plateaus at 93 percent around 175 epochs \n",
    "\n",
    "The loss function exponentially decreases with increasing number of epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHDjO3fZTPrl"
   },
   "source": [
    "### Saving Weights to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Aa1aRyrqTPrl"
   },
   "outputs": [],
   "source": [
    "file = open('weights', 'wb+')\n",
    "pickle.dump(parameters, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n",
    "2. https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Q4rev (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
